


--- 
title: "TMA4315 Exercise 1: Linear models for Gaussian data" 
subtitle: "Norunn Ahdell Wankel, Kristine Lund Mathisen and Yngvild Hamre" 
date: "September 2017" 
output: 
 prettydoc::html_pretty:
   theme: architect
   highlight: github
#  pdf_document:
#   toc: false
#   toc_depth: 2

---

```{r setup, include=FALSE} 
knitr::opts_chunk$set(echo = TRUE, tidy = TRUE) 
```


## Loading the data set
The code available in the assignment text was used for loading the data set.

```{r cars}
library(car)
data(SLID, package = "car")
SLID <- SLID[complete.cases(SLID), ]

library(GGally)
```

## Part 1: Explanatory analysis of the data set

```{r investigate, echo=FALSE, warning=FALSE, error=FALSE, message=FALSE}
ggpairs(SLID)
```

Comment on the relationships between some of the variables:  
**Wage and other variables:**  
*Wage:* The distrution of wage is skewed to the left. There are few people who earn a lot, but some earn much more than the majority.  
*Wage and eduction:* These variables has a correlation of 0.306, suggesting that the more education a person has, the more she can expect to earn.  
*Wage and age:* These variables have a correlation of 0.36. Old persons often earn more than younger persons.  
*Wage and sex:* It seems like women often earn less than men, the distribution of wages is more skewed to the left for women. This is confirmed by the box-plot, showing a lower median-wage for women than men. Also, the right tail of the distribution is longer for men than for women: It seems like almost no women is in the highest-income group.   
*Wages and language:* It does not seem to be a large difference in wage for different languages.  
**Education and other variables: ** 
*Education:* Most people seem to have about 10-18 years of education. Very few have less than 7 years of education, which suggests mandatory school for about 7 years for most people.   
*Education and age:* Has a correlation of -0.106. This is weak, but might be due to the oldest people not having had the same opportunites as young people have today when it comes to education.  
*Education and sex:* It seems like female and men have about the same level of education. THere are a few women with almost no education, while no men seem to have that little education.  
*Education and language:* French-speaking people have a slightly lower median education than the other groups.  
**Age and other variables: ** 
*Age:* The largest bulk of people in the data set is between 30 and 50 years old. Few are above 60.  
*Age and sex:* No interesting observations
*Age and language:* The "other" language group have a higher median age.   
**Sex and other variables:**   
*Sex:* It seems like there is an almost equal number of men and women in the data set.  
*Sex and language:* It seems like there are more french-speaking men than women.  
**Language: ** 
*Language* There are mostly english-speaking people in the data set.   

## Part 1 continued: Assumptions for a multiple linear regression   
The assumptions we need make about the data if we want to perform a multiple linear regression analysis to fit a model $Y = X\beta +\epsilon$ are:    
**Linearity of covariates:** $Y = X\beta + \epsilon$. The continuous variable Y can be written as a linear combination of covariates.  
**Homoscedastic error variance:** Cov($\epsilon$)=$\sigma^2$. All the errors have a constant error variance.  
**Uncorrelated errors**: Cov($\epsilon_i,\epsilon_j$)=0. There is no correlation between the errors.   
**Additivity of errors:** $Y = X \beta + \epsilon$.   
**Expectation of the errors:**  E($\epsilon$) = 0.  The expected value of the errors are zero.    
**Independence of covariates** If the covariates are stochastic, errors and stochastic covariates are assumed to be independent. The variance and expected value of the errors do not depend on the covariate.  
**Full rank** The design matrix, X, has full rank.  
**Assumption of normality:** $\epsilon \sim$N(0,$\sigma^2$). The classical normal linear regression model is obtained if  this holds.  
  

## Part 2: Linear regression with the mylm package 
**Task a**   
In this task, we should fill-in the missing parts in the mylm function required to calculate the estimates of the coefficients. This was done by using the formula $\hat{\beta}=(X^TX)^{-1}X^T Y$. The R-code snippet is shown below.
```{r betahat, eval=FALSE}
  #Finding estimated coefficients
  betahat=((solve(t(X)%*%X))%*%t(X))%*%y
```
To check the results, a print-function was implemented as follows:
```{r print, eval=FALSE}
  print.mylm <- function(object, ...){
  # Code here is used when print(object) is used on objects of class "mylm"
  cat('Coefficients:\n')
  print.default(x=t(object$coeffs),digits=4,print.gap = 4)
}
```
The output of the command print(model1) then looks like this: 
HA MED SLIKT OGSÅ???? 

**Task b**  
To estimate the covariance matrix for the parameter estimates, the formula Var($\hat{\beta}$)=$\sigma^2 X^TX^{-1}$ was used. ${\sigma^2}$ is estimated by the REML estimator  $\hat{\sigma^2}=\frac{1}{n-p}(Y-X\hat{\beta})^T(Y-X\hat{\beta})=\frac{SSE}{n-p}$. The code snippet below shows the implementation.

```{r covmatrix, eval=FALSE}
#Finding the estimated sigma
  sigma2hat=(1/(n-p))*t(y-X%*%betahat)%*%(y-X%*%betahat)
  sigmahat=sqrt(sigma2hat)

  #Finding the covariance matrix of the estimated coefficients
  varbetahat=sigma2hat[1]*solve(t(X)%*%X)
```
The standard error of the intercept and regression coefficient for this model is given by the square root of the corresponding diagonal element of covariance matrix for the parameter estimates. The code for finding the standard errors is

```{r stderrors, eval=FALSE}
  #Finding the standard errors of the estimated coefficients
  stderr=(sqrt(diag(varbetahat)))
  stderror<-c(stderr[[1]],stderr[[2]])
```

The significance of the regression coefficients can be found using a z-test, as $T_j=\frac{\hat{\beta_j}-\beta_j}{\sqrt(c_jj)\sigma}\sim t_{n-p}$ approaches the normal distribution when n is large, as in this case. (SANT????) The null hypothesis is that $\beta_j =0$, and $c_{jj}$ is the jj'th element of $(X^TX)^{-1}$. The test statistic thus becomes $\frac{\hat{\beta_j}}{SE(\hat{\beta_j})}$, where $SE(\hat{\beta_j})$ is the standard error of $\hat{\beta}$. The test statistic and corresponding p-value is found in R as follows:
```{r testing, eval=FALSE}
  #Testing the significance of the coefficients 
  z=numeric(p)
  pvalue=numeric(p)
  for(i in 1:p){
    z[i]<- (betahat[i])/(stderror[i])
    pvalue[i]= 2*pnorm(-abs(z[i]))
  }
```

The function summary was adjusted to includea similar table of tests of significance as summary used on an lm object. See the full code of the summary function for the implementation. HUSK  

The interpretation of the parameter estimates $\hat{\beta_0=4.938}$ and $\hat{\beta_1}= 0.7946$ is that the mean wage of people with no education is 4.938, and the expected pay-off per additional year of education is 0.7946.  

**Task c**
Task C was to implement a plot function for the mylm class that makes a scatter plot with the fitted values on the x-axis and the raw residuals on the y-axis. The fitted values are $\hat{Y}=X\hat{\beta}$, while the raw residuals are defined as $Y-\hat{Y}=Y-X\hat{\beta}$. The following code snippet was added to the mylm-function, and the values returned form the function in order to be plotted. 
```{r rawres, eval=FALSE}
  #Finding fitted values and raw residuals
  fittedvalues=X%*%betahat
  rawresiduals=y-X%*%betahat

```
The plot function was implemented as follows: FÅ MED PLOTT OGSÅ HER
```{r plot, eval=FALSE}
plot.mylm <- function(object, ...){
  # Code here is used when plot(object) is used on objects of class "mylm"
  library(ggplot2)
  dat <- data.frame(    yvar = object$rres,  xvar = object$fv)
  ggplot(dat, aes(x=xvar, y=yvar)) +
    geom_point(shape=1)+labs(x = "Fitted values")  +labs(y = "Raw residuals")

}

```
**Task d**
What is the residual sum of squares (SSE) for this model is $SSE=\sum(Y_i-\hat{Y_i})^2=(Y-X\hat{\beta})^T(Y-X\hat{\beta})=225303$ and the degrees of freedom is $n-p=4012$ for this model, as $p$ is the number of estimated coefficients including the intercept and $n$ is the number of observations.  
The total sum of squares, SST=$\sum(Y_i-\bar{Y})^2$ is 248685.9 for this model.
These values were found in R using the following code from the mylm-function: 
```{r SSE, eval=FALSE}
  #Finding SSE, SST 
  SSE=t(rawresiduals)%*%rawresiduals
  SST=t(y-mean(y))%*%(y-mean(y))
  SSR=SST-SSE
```
The significance of the regression can be tested using a $\chi^2$-test, as $F=\frac{(n-p)(SSE_{H_0}-SSE)}{r(SSE)} \sim f_{r,n-p}$, and $rF_{r,n-p}\sim\chi^2_r$, where $SSE_{H_0}$ is the SSE of the model where $H_0$ is true, and $H_0$ is $\beta_j=0  \forall  j$. 
This test is performed in the anova-function as follows:
```{r SSEtest, eval=FALSE}
    Fobs=round(((n-p)/r)*(model[[numComp]]$SSE-model[[length(comp)]]$SSE),2)
    pvalueF=pchisq(r*Fobs,r,lower.tail=FALSE)
```
What is the relationship between the χ2χ2- and zz-statistic?
The relationship between the $\chi^2$-statistic and the z-statistic is that when r=1, the square root of the $\chi^2$-statistic follows a half-normal distribution.  The critical value is then found as FORKLAR HER
```{r ztest, eval=FALSE}
  zstat=sqrt(r*Fobs)
  pvaluez=2*pnorm(-abs(zstat))
```
In the case when only education was tested, a p-value of 1.496245e-92 was found for both test methods.  
SKRIV OM ANOVA HER

**Task e**  
The “coefficient of determination, $R^2$, is given by $R^2=\frac{SSR}{SST}$, the proportion of variability explained by the model divided by the total variability. For this model, the value is 0.09402572. As $R^2$ is a value between 0 and 1 where 0 means that the nothing is explained, and 1 is that everything is explained, a value of 0.09 indicates that education does not explain much of variability in wages.   

**Task f**  
Pearson's linear correlation coefficient is given by $r_{xy}=\frac{\sum(x_i-\bar{x})\sum(y_i-\bar{y})}{\sqrt(\sum(x_i-\bar{x})^2)\sqrt(\sum(y_i-\bar{y})^2)}$, and was implemented in R as follows:
```{r pearson, eval=FALSE}
  #Finding Pearsons correlation coefficient
  Pearson=(t(X[,2]-mean(X[,2]))%*%(y-mean(y)))/(sqrt(SST)*sqrt(t(X[,2]-mean(X[,2]))%*%(X[,2]-mean(X[,2]))))
```
The value for this model was found to be 0.3066361. $r_{xy}$ is a measure of the linear correlation between X and Y, where -1 indicates total negative correlation, 0 is no correlation, and 1 is total positive correlation. A value of 0.31 can be interpreted as a weak, positive correlation between education and wage. 
