


--- 
title: "TMA4315 Exercise 1: Linear models for Gaussian data" 
subtitle: "Norunn Ahdell Wankel, Kristine Lund Mathisen and Yngvild Hamre" 
date: "September 2017" 
output: 
 prettydoc::html_pretty:
   theme: architect
   highlight: github
#  pdf_document:
#   toc: false
#   toc_depth: 2

---

```{r setup, include=FALSE} 
knitr::opts_chunk$set(echo = TRUE, tidy = TRUE) 
```


```{r cars,echo=FALSE,message=FALSE}
library(car)
library(GGally)
library(mylm)
data(SLID, package = "car")
SLID <- SLID[complete.cases(SLID), ]
#Create models for use later in report
model1=mylm(wages ~ education, data = SLID)
model2=lm(wages ~ education, data = SLID)

```

## Part 1: Explanatory analysis of the data set

```{r investigate, echo=FALSE, warning=FALSE, error=FALSE, message=FALSE}
ggpairs(SLID)
```

Comment on the relationships between some of the variables:  
**Wage and other variables:**  
*Wage:* The distrution of wage is skewed to the left. There are few people who earn a lot, but some earn much more than the majority.  
*Wage and eduction:* These variables has a correlation of 0.306, suggesting that the more education a person has, the more she can expect to earn.  
*Wage and age:* These variables have a correlation of 0.36. Old persons often earn more than younger persons.  
*Wage and sex:* It seems like women often earn less than men, the distribution of wages is more skewed to the left for women. This is confirmed by the box-plot, showing a lower median-wage for women than men. Also, the right tail of the distribution is longer for men than for women: It seems like almost no women is in the highest-income group.   
*Wages and language:* It does not seem to be a large difference in wage for different languages.  
**Education and other variables: ** 
*Education:* Most people seem to have about 10-18 years of education. Very few have less than 7 years of education, which suggests mandatory school for about 7 years for most people.   
*Education and age:* Has a correlation of -0.106. This is weak, but might be due to the oldest people not having had the same opportunites as young people have today when it comes to education.  
*Education and sex:* It seems like female and men have about the same level of education. THere are a few women with almost no education, while no men seem to have that little education.  
*Education and language:* French-speaking people have a slightly lower median education than the other groups.  
**Age and other variables: ** 
*Age:* The largest bulk of people in the data set is between 30 and 50 years old. Few are above 60.  
*Age and sex:* No interesting observations
*Age and language:* The "other" language group have a higher median age.   
**Sex and other variables:**   
*Sex:* It seems like there is an almost equal number of men and women in the data set.  
*Sex and language:* It seems like there are more french-speaking men than women.  
**Language: ** 
*Language* There are mostly english-speaking people in the data set.   

## Part 1 continued: Assumptions for a multiple linear regression   
The assumptions we need make about the data if we want to perform a multiple linear regression analysis to fit a model $Y=X\beta+\epsilon$ are:    
**Linearity of covariates:**$Y=X\beta+\epsilon$. The continuous variable Y can be written as a linear combination of covariates.  
**Homoscedastic error variance:** Cov($\epsilon$)=$\sigma^2$. All the errors have a constant error variance.  
**Uncorrelated errors**: Cov($\epsilon_i,\epsilon_j$)=0. There is no correlation between the errors.   
**Additivity of errors:** $Y=X\beta+\epsilon$.
**Expectation of the errors:**  E($\epsilon$) = 0.  The expected value of the errors are zero.    
**Independence of covariates** If the covariates are stochastic, errors and stochastic covariates are assumed to be independent. The variance and expected value of the errors do not depend on the covariate.  
**Full rank** The design matrix, X, has full rank.  
**Assumption of normality:** $\epsilon \sim$ Nn(0,$\sigma^2$). The classical normal linear regression model is obtained if  this holds.  
  

## Part 2: Linear regression with the mylm package 
**Task a**   
In this task, we should fill-in the missing parts in the mylm function required to calculate the estimates of the coefficients. This was done by using the formula $\hat{\beta}=(X^TX)^{-1}X^T Y$.   

To check the results, a print-function was implemented. Model1 now is the model found using our mylm-package, while model2 is the model found using the lm-package. These will be used throughout task 2 to compare our functions with the built-in functions. Printing the models yields
```{r print}
  print(model1)
print(model2)
```
so the results seems reasonable.

**Task b**  
To estimate the covariance matrix for the parameter estimates, the formula Var($\hat{\beta}$)=$\sigma^2 X^TX^{-1}$ was used. ${\sigma^2}$ is estimated by the REML estimator  $\hat{\sigma^2}=\frac{1}{n-p}(Y-X\hat{\beta})^T(Y-X\hat{\beta})=\frac{SSE}{n-p}$. 
The standard error of the intercept and regression coefficient for this model is given by the square root of the corresponding diagonal element of covariance matrix for the parameter estimates.  

The significance of the regression coefficients can be found using a z-test, as $T_j=\frac{\hat{\beta_j}-\beta_j}{\sqrt(c_jj)\sigma}\sim t_{n-p}$ approaches the normal distribution when n is large, as in this case. The null hypothesis is that $\beta_j =0$, and $c_{jj}$ is the jj'th element of $(X^TX)^{-1}$. The test statistic thus becomes $\frac{\hat{\beta_j}}{SE(\hat{\beta_j})}$, where $SE(\hat{\beta_j})$ is the standard error of $\hat{\beta}$. The p-value is 2 times the p-value returned by the function p-norm, as we use a 2-sided test in this case.  

The function summary was adjusted to include a similar table of tests of significance as summary used on an lm object. Using summary on model1 and model 2 yields

```{r summary}
  summary(model1)
summary(model2)
```


The interpretation of the parameter estimates $\hat{\beta_0=4.938}$ and $\hat{\beta_1}= 0.7946$ is that the mean wage of people with no education is 4.938, and the expected pay-off per additional year of education is 0.7946.  

**Task c**
Task C was to implement a plot function for the mylm class that makes a scatter plot with the fitted values on the x-axis and the raw residuals on the y-axis. The fitted values are $\hat{Y}=X\hat{\beta}$, while the raw residuals are defined as $Y-\hat{Y}=Y-X\hat{\beta}$. The plot functions yields the following plot: 

```{r plot}
plot(model1)

```


**Task d**  
What is the residual sum of squares (SSE) for this model is $SSE=\sum(Y_i-\hat{Y_i})^2=(Y-X\hat{\beta})^T(Y-X\hat{\beta})=225303$ and the degrees of freedom is $n-p=4012$ for this model, as $p$ is the number of estimated coefficients (including the intercept) and $n$ is the number of observations.  
The total sum of squares, SST=$\sum(Y_i-\bar{Y})^2$ is 248685.9 for this model.  

The significance of the regression can be tested using a $\chi^2$-test, as $F=\frac{(n-p)(SSE_{H_0}-SSE)}{r(SSE)} \sim f_{r,n-p}$, and $rF_{r,n-p}\sim\chi^2_r$, where $SSE_{H_0}$ is the SSE of the model where $H_0$ is true, and $H_0$ is $\beta_j=0  \forall  j$. 
This test is performed in the anova-function as follows:
```{r SSEtest, eval=FALSE}
    Fobs=round(((n-p)/r)*(model[[numComp]]$SSE-model[[length(comp)]]$SSE),2)
    pvalueF=pchisq(r*Fobs,r,lower.tail=FALSE)
```

The relationship between the $\chi^2$-statistic and the z-statistic is that when r=1, the absolute value of the square root of the $\chi^2$-statistic follows a half-normal distribution.  The critical value is then found as twice the p-value found using a normal distribution.  
```{r ztest, eval=FALSE}
  zstat=sqrt(r*Fobs)
  pvaluez=2*pnorm(-abs(zstat))
```
In the case when only education was tested, a p-value of 1.496245e-92 was found for both test methods.  

A function, anova, using sequentiel ANOVA, was then implemented. Sequential ANOVA means starting with a model with only the intercept, then adding one covariate, then registering the decrease in SSE as each term in the regression formula is added in turn. The F-statistic $F=\frac{(n-p)(SSE(\beta_i+1|\beta_i,\beta_{i-1},...,\beta_0))}{r(SSE(\beta_k,\beta_{k-1},...,\beta_0))} \sim f_{r,n-p}$ is then used to check if the change in SSE is significant.   
Note: For checking if $\beta_1$ is significant the $SSE(\beta_1|\beta_0)$ is equal to the SSR for the model including $\beta_1$.  
Using anova on model1 and model2 yields the following results:
```{r anova}
anova(model1)
anova(model2)

```

**Task e**  
The “coefficient of determination”, $R^2$, is given by $R^2=\frac{SSR}{SST}$, the proportion of variability explained by the model divided by the total variability. For this model, the value is 0.09402572. As $R^2$ is a value between 0 and 1 where 0 means that the nothing is explained, and 1 is that everything is explained, a value of 0.09 indicates that education does not explain much of variability in wages.   

**Task f**  
Pearson's linear correlation coefficient is given by $r_{xy}=\frac{\sum(x_i-\bar{x})\sum(y_i-\bar{y})}{\sqrt(\sum(x_i-\bar{x})^2)\sqrt(\sum(y_i-\bar{y})^2)}$, and was implemented in R as follows:
```{r pearson, eval=FALSE}
  #Finding Pearsons correlation coefficient
  Pearson=(t(X[,2]-mean(X[,2]))%*%(y-mean(y)))/(sqrt(SST)*sqrt(t(X[,2]-mean(X[,2]))%*%(X[,2]-mean(X[,2]))))
```
The value for this model was found to be 0.3066361. $r_{xy}$ is a measure of the linear correlation between X and Y, where -1 indicates total negative correlation, 0 is no correlation, and 1 is total positive correlation. A value of 0.31 can be interpreted as a weak, positive correlation between education and wage. 
